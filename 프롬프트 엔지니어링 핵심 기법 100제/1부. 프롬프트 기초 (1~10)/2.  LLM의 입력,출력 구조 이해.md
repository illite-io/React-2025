# 2. LLM의 입력/출력 구조 이해

## 입력 구조

LLM은 크게 **토큰 단위**로 입력을 처리합니다.
토큰(Token)은 보통 단어, 단어의 일부, 또는 기호 단위로 나뉩니다.
즉, 모델은 글자 그대로 문장을 처리하는 게 아니라, 이를 **숫자로 인코딩된 토큰 시퀀스**로 바꿔서 계산합니다.

* **사용자 입력** → 토큰화(Tokenization) → 모델 내부 연산
* **맥락(Context)**: 최근 대화나 시스템 지시(system prompt)가 모두 합쳐져 입력으로 들어감
* **입력 제한**: 모델마다 context window 크기(예: 8k, 32k, 200k 토큰 등)가 존재

---

## 출력 구조

출력도 동일하게 토큰 단위로 생성됩니다.
모델은 한 번에 전체 문장을 내보내는 게 아니라, **다음에 올 확률이 가장 높은 토큰을 하나씩 예측**하여 이어 붙입니다.

* 이전 토큰들을 기반으로 **다음 토큰 확률 분포** 계산
* 확률이 가장 높은 토큰 선택 (또는 샘플링)
* 최종적으로 문자열(텍스트)로 변환되어 사용자에게 전달

---

## 핵심 특징

1. **순차적 생성**
   모델은 단어 단위가 아니라 토큰 단위로 하나씩 뽑아내며 응답을 구성

2. **맥락 의존적**
   같은 질문이라도 입력 앞부분(시스템 지시, 대화 이력)에 따라 전혀 다른 답변을 생성

3. **제어 가능**

   * Temperature: 창의성/랜덤성 제어
   * Top-p (Nucleus Sampling): 상위 확률 집합에서 선택
   * Max tokens: 출력 길이 제한

---

## 간단한 예시

```text
입력: "사과는 영어로?"
→ 토큰화 후 모델 입력
→ 모델은 "apple"의 a → p → p → l → e 순으로 토큰을 하나씩 예측
→ 최종 출력: "apple"
```

---
